{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a121b0",
   "metadata": {},
   "source": [
    "# Introduction to Dask and Parquet Workshop\n",
    "\n",
    "\n",
    "**What you will learn:**\n",
    "- What is Dask and why use it for large datasets?\n",
    "- Core techniques including lazy evaluation and using `.compute()`\n",
    "- How to work with CSV and Parquet files for efficient data handling\n",
    "- Visualizing Dask’s computation graph with `.visualize()`\n",
    "- The \"start small, go big\" approach for scaling analyses\n",
    "- Hands-on challenges to solidify your understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c7751",
   "metadata": {},
   "source": [
    "## What is Dask?\n",
    "\n",
    "**Dask** is a Python library that enables parallel computing and out-of-core processing by splitting large datasets into smaller partitions. Its API mirrors that of Pandas, which makes it easier to scale your workflows without learning a completely new system.\n",
    "\n",
    "Below is a simple example that converts a small Pandas DataFrame into a Dask DataFrame\n",
    "\n",
    "Lazy Evaluation and `.compute()`**\n",
    "\n",
    "### Lazy Evaluation and `.compute()`\n",
    "\n",
    "Dask employs **lazy evaluation**. This means that when you perform operations on a Dask DataFrame, it builds a task graph of computations without immediately executing them. Actual computation only happens when you call `.compute()`.\n",
    "\n",
    "For example, consider calculating the mean of a column:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Create a small Pandas DataFrame\n",
    "small_df = pd.DataFrame({'col1': [1, 2, 3], 'col2': [10, 20, 30]})\n",
    "# Convert it to a Dask DataFrame with 1 partition (since it's tiny)\n",
    "ddf = dd.from_pandas(small_df, npartitions=1)\n",
    "\n",
    "# Perform an operation on the Dask DataFrame (lazy, no computation yet)\n",
    "result = ddf['col2'].mean()\n",
    "print(\"Dask result (lazy):\", result)         # This is a lazy Dask Scalar, not a real number yet\n",
    "\n",
    "# Now actually compute the result\n",
    "real_result = result.compute()\n",
    "print(\"Computed result:\", real_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09665ed0",
   "metadata": {},
   "source": [
    "## What Are Partitions in Dask?\n",
    "\n",
    "In Dask, **partitions** are smaller chunks of your larger dataset. Think of them as pieces of a large puzzle:\n",
    "\n",
    "- **Division of Data:** When you load a dataset (for example, from a CSV or a Parquet file) into a Dask DataFrame, Dask splits the data into multiple partitions. Each partition is essentially a smaller Pandas DataFrame that holds just a subset of the data.\n",
    "- **Parallel Processing:** Because each partition is independent, Dask can process them in parallel. This means that operations on your DataFrame (like filtering, aggregating, etc.) can be executed on each partition at the same time, making the process faster.\n",
    "- **Memory Efficiency:** Working with many partitions allows you to process datasets that are larger than your machine's memory. Only one partition (or a few) is loaded at any moment, helping to avoid memory overload.\n",
    "- **Lazy Evaluation:** Dask builds a task graph based on these partitions. For instance, when you call an operation like `.mean()`, Dask plans to compute the mean on each partition first, and then combine the results. The whole process is done lazily (i.e., it won't compute until you explicitly trigger it with `.compute()` or similar).\n",
    "\n",
    "**In summary:**  \n",
    "Partitions are Dask's way of breaking down a large dataset into smaller, manageable chunks that can be processed in parallel. This approach is what makes Dask so powerful for handling big data with a familiar, Pandas-like syntax.\n",
    "\n",
    "### In this example, we:\n",
    "- Create a Dask DataFrame with **10 partitions** from a Pandas DataFrame containing 100,000 rows.\n",
    "- Apply a filter to keep only rows where `value1` is positive.\n",
    "- Group the filtered data by the `group` column and calculate the mean of `value1` for each group.\n",
    "- Visualize the computation graph using `.visualize()`, which will show a more complex graph because it processes multiple partitions in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fdcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample Pandas DataFrame with 100,000 rows\n",
    "N=20000\n",
    "df = pd.DataFrame({\n",
    "    'id': np.arange(1, N+1),\n",
    "    'value1': np.random.randn(N),    # Simulated continuous measurement\n",
    "    'group': np.random.choice(['A', 'B', 'C'], N)  # Simulated categorical groups\n",
    "})\n",
    "\n",
    "# Convert the Pandas DataFrame to a Dask DataFrame with 10 partitions\n",
    "ddf = dd.from_pandas(df, npartitions=10)\n",
    "print(\"Number of partitions:\", ddf.npartitions)\n",
    "\n",
    "# Filter the DataFrame to keep only rows where 'value1' is positive\n",
    "filtered_ddf = ddf[ddf['value1'] > 0]\n",
    "\n",
    "# Group the filtered data by 'group' and compute the mean of 'value1' for each group\n",
    "grouped = filtered_ddf.groupby('group')['value1'].mean()\n",
    "\n",
    "# Compute the final result (this triggers the execution of the task graph)\n",
    "result = grouped.compute()\n",
    "print(\"Mean of 'value1' for each group (after filtering):\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42142af",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### More artificial datasets\n",
    "\n",
    "In scientific workflows, you often work with very large datasets. In this example, we simulate a large dataset by creating four CSV files, each containing 250,000 rows. These files mimic experimental batches or repeated measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272adf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 250_000  # number of rows per file\n",
    "for i in range(4):\n",
    "    start_id = i * n + 1\n",
    "    end_id = (i+1) * n\n",
    "    # Generate data for this chunk\n",
    "    ids = np.arange(start_id, end_id + 1)  # inclusive of end\n",
    "    value1 = np.random.randn(n)           # n random floats (normal distribution)\n",
    "    value2 = np.random.randint(0, 100, size=n)  # n random ints between 0 and 99\n",
    "    groups = np.random.choice(['A', 'B'], size=n)  # n random choices of 'A' or 'B'\n",
    "    chunk_df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'value1': value1,\n",
    "        'value2': value2,\n",
    "        'group': groups\n",
    "    })\n",
    "    filename = f\"data_part{i}.csv\"\n",
    "    chunk_df.to_csv(filename, index=False)\n",
    "    print(f\"Wrote {filename} with {len(chunk_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c39981",
   "metadata": {},
   "source": [
    " Quick Challenge – Summarize a Large CSV with Dask\n",
    "\n",
    "**Challenge:** Load your simulated CSV files into a Dask DataFrame and compute summary statistics, such as:\n",
    "- The mean of `value1`\n",
    "- The total row count\n",
    "\n",
    "**Steps:**\n",
    "1. Use `dd.read_csv(\"data_part*.csv\")` to load all CSV files.\n",
    "2. Preview the data using `.head()`.\n",
    "3. Compute the mean of `value1` and the count of `id` using `.mean()` and `.count()`, followed by `.compute()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54824c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using Dask (reading all CSV parts)\n",
    "ddf = dd.read_csv('data_part*.csv')\n",
    "\n",
    "# Then, repartition the DataFrame to a desired number of partitions\n",
    "ddf = ddf.repartition(npartitions=8)\n",
    "\n",
    "# Peek at the data (this will print a few rows from the first partition)\n",
    "print(\"Preview of data:\\n\", ddf.head())\n",
    "\n",
    "# Compute summary statistics: mean of value1 and total row count\n",
    "mean_val = ddf[\"value1\"].mean()  # lazy Dask scalar (operation defined, not computed yet)\n",
    "row_count = ddf[\"id\"].count()    # lazy Dask scalar (this counts non-null 'id' values)\n",
    "\n",
    "# Now trigger the computation to obtain the actual results\n",
    "mean_val_result = mean_val.compute()\n",
    "row_count_result = row_count.compute()\n",
    "\n",
    "print(\"Mean of value1:\", mean_val_result)\n",
    "print(\"Total rows:\", row_count_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5621026",
   "metadata": {},
   "source": [
    "## Converting CSV to Parquet\n",
    "\n",
    "While CSV is a common format, **Parquet** is a more efficient columnar data storage format that is well suited for large datasets. Parquet files are typically faster to read and write, support compression, and store schema information.\n",
    "\n",
    "In this example, we convert our CSV dataset (loaded into a Dask DataFrame) to Parquet files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8769bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Dask DataFrame to Parquet files\n",
    "ddf.to_parquet('data_parquet', write_index=False)\n",
    "\n",
    "# Check the output directory\n",
    "import os\n",
    "print(\"Parquet files:\", os.listdir('data_parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77694d4",
   "metadata": {},
   "source": [
    "## Reading Parquet Files with Dask\n",
    "\n",
    "After converting CSV files to Parquet, loading the data back is very efficient. Parquet’s embedded schema helps Dask quickly reconstruct the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d49dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet files as a Dask DataFrame\n",
    "ddf2 = dd.read_parquet('data_parquet')\n",
    "print(type(ddf2))  # Should be a Dask DataFrame\n",
    "# Check the schema and first few rows\n",
    "print(\"Data types:\", ddf2.dtypes)\n",
    "print(\"Preview of Parquet data:\\n\", ddf2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb284371",
   "metadata": {},
   "source": [
    "### Visualizing the Dask Computation Graph\n",
    "\n",
    "One of Dask’s powerful features is the ability to **visualize its task graph**. This helps you understand how Dask schedules and performs computations in parallel.\n",
    "\n",
    "In this example, we visualize the computation graph for calculating the mean of the `value1` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1f7e2",
   "metadata": {},
   "source": [
    "### Start Small, Go Big\n",
    "\n",
    "A common strategy in data processing is to **start small**:\n",
    "- Test your code on a subset of your data (using `.head()` or reading a single file) to catch errors quickly.\n",
    "- Once verified, scale your analysis up to the full dataset.\n",
    "\n",
    "This approach minimizes wasted time and avoids long computations during development. Dask’s lazy evaluation allows you to work on a small sample and later execute the entire pipeline seamlessly.\n",
    "\n",
    "Remember: Always validate your code on a small sample before scaling up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# 1. Read the CSV files into a Dask DataFrame\n",
    "ddf = dd.read_csv('data_part*.csv')\n",
    "\n",
    "# 2. Write the Dask DataFrame to Parquet files (in a new directory)\n",
    "ddf.to_parquet('challenge_parquet', write_index=False)\n",
    "\n",
    "# 3. Read back the Parquet files into a new Dask DataFrame\n",
    "ddf_parquet = dd.read_parquet('challenge_parquet')\n",
    "\n",
    "# 4. Verify the number of rows and columns match\n",
    "rows_csv = ddf[\"id\"].count().compute()\n",
    "rows_parquet = ddf_parquet[\"id\"].count().compute()\n",
    "print(\"Rows in original CSV data:\", rows_csv)\n",
    "print(\"Rows in Parquet data:     \", rows_parquet)\n",
    "\n",
    "cols_csv = set(ddf.columns)\n",
    "cols_parquet = set(ddf_parquet.columns)\n",
    "print(\"Columns in original CSV data:\", cols_csv)\n",
    "print(\"Columns in Parquet data:     \", cols_parquet)\n",
    "print(\"Same number of rows in both? \", rows_csv == rows_parquet)\n",
    "print(\"Same columns in both?        \", cols_csv == cols_parquet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
