{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas for Scientific Data (Animal Tracking) Workshop\n",
    "\n",
    "We will explore key pandas concepts and patterns commonly used in scientific data analysis, using biological examples (like animal observation data). This notebook is structured as a hands-on code-along, so make sure to run the code cells and experiment with the examples.\n",
    "\n",
    "**Topics Covered:**\n",
    "1. **Multi-indexing**: Creating multi-level indices, accessing data, resetting indices.\n",
    "2. **Merging DataFrames**: Combining data with inner, outer, left, and right joins.\n",
    "3. **GroupBy Operations**: Aggregating data by groups (including custom functions).\n",
    "4. **Pivot and Pivot Tables**: Reshaping data from long to wide format.\n",
    "5. **Efficiency Pitfalls**: Why `DataFrame.apply` can be slow.\n",
    "6. **Looping Pitfalls**: Why using Python `for` loops or `iterrows` is inefficient in pandas.\n",
    "7. **Chained Indexing Caution**: Why chained indexing should be avoided and how to do it right.\n",
    "8. **Challenges**: Two end-of-notebook exercises to practice these concepts, with solutions.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Multi-Indexing in pandas\n",
    "\n",
    "**Multi-indexing** allows a DataFrame to have multiple index (row) levels. This is useful for hierarchical or grouped data (e.g., experiments with multiple conditions, animal trajectories over time, etc.). With multi-index, you can index your data by two or more keys for more complex data relationships.\n",
    "\n",
    "**Key concepts:**\n",
    "- You can create a multi-index by setting multiple columns as the index.\n",
    "- Access data by providing a tuple of index values (or using slice notation for ranges).\n",
    "- Multi-index can be converted back to regular columns with `reset_index()` when needed.\n",
    "\n",
    "Let's see multi-indexing in action with an example dataset of animal observations over multiple days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    'Animal': ['Lion', 'Lion', 'Zebra', 'Zebra', 'Zebra'],\n",
    "    'Date': pd.date_range('2021-01-01', periods=5, freq='D'),\n",
    "    'Speed': [10, 12, 8, 9, 7],      # e.g. speed recorded (m/s)\n",
    "    'Strength': [7, 8, 6, 7, 5]      # e.g. strength score (arbitrary units)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "display(df)  # Show the DataFrame in tabular form\n",
    "\n",
    "# Create a multi-index on Animal and Date\n",
    "df_mi = df.set_index(['Animal', 'Date'])\n",
    "print(\"\\nDataFrame with Multi-Index (Animal, Date):\")\n",
    "display(df_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: Does Pandas work like numpy? is the new dataframe also altering the original one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the DataFrame now has a hierarchical index: the first level is **Animal** and the second level is **Date**. Each row is identified by a pair `(Animal, Date)`. The `display(df_mi)` output shows the two index levels on the left.\n",
    "\n",
    "### Accessing data in a Multi-Index DataFrame\n",
    "\n",
    "You can use `.loc` with a tuple to access specific entries or slices in a multi-index DataFrame:\n",
    "- To get all data for a specific animal (all dates for that animal).\n",
    "- To get a specific animal on a specific date.\n",
    "- To get a range or subset of index values (e.g., all animals in a date range, or multiple animals).\n",
    "\n",
    "Let's try some examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access all observations for a specific animal:\n",
    "print(\"All observations for Lion:\")\n",
    "display(df_mi.loc['Lion'])\n",
    "\n",
    "# Access a specific animal on a specific date (using a tuple for multi-index):\n",
    "print(\"\\nSpeed of Zebra on 2021-01-04:\")\n",
    "value = df_mi.loc[('Zebra', '2021-01-04'), 'Speed']\n",
    "print(f\"Zebra on 2021-01-04 - Speed: {value}\")\n",
    "\n",
    "# Accessing a range or multiple index values:\n",
    "print(\"\\nAll observations for Zebra from 2021-01-04 onwards:\")\n",
    "display(df_mi.loc[('Zebra', slice('2021-01-04', None)), :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples above:\n",
    "- `df_mi.loc['Lion']` returns all rows where the first index level is 'Lion' (all Lion observations).\n",
    "- `df_mi.loc[('Zebra', '2021-01-04'), 'Speed']` returns the Speed value for Zebra on Jan 4, 2021.\n",
    "- We can use `slice` for slicing in a multi-index. For instance, `('Zebra', slice('2021-01-04', None))` selects Zebra for all dates from '2021-01-04' to the end.\n",
    "- There are alternatives to not using `slice`, such as  `pd.IndexSlice`:\n",
    "\n",
    "``` python\n",
    "idx = pd.IndexSlice\n",
    "df_mi.loc[('Zebra', idx['2021-01-04':]), :]\n",
    "\n",
    "```\n",
    "### Resetting the index\n",
    "\n",
    "If you no longer need the multi-index, or want to revert it back to columns, use `reset_index()`. This will turn the index levels back into ordinary columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the multi-index to get back to a regular DataFrame\n",
    "df_reset = df_mi.reset_index()\n",
    "print(\"After resetting index:\")\n",
    "display(df_reset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame is back to its original form with `Animal` and `Date` as regular columns.\n",
    "\n",
    "Multi-indexes are powerful for handling hierarchical data, but they can be complex. In many cases, you might use groupby or pivot tables (which we'll cover next) instead of maintaining a multi-index permanently. Still, it's important to know how to work with multi-indexes when they arise (for example, after certain groupby operations or when dealing with multi-dimensional data).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Merging DataFrames (Joins)\n",
    "\n",
    "Often, you'll have related data split across multiple tables/DataFrames. **Merging** (or **joining**) lets you combine these DataFrames on a common key (or keys), similar to how you would join tables in a database or merge in Excel.\n",
    "\n",
    "Pandas offers the `pd.merge()` function (or `DataFrame.merge` method) to perform joins. The main types of joins are:\n",
    "- **Inner join**: only keep rows with keys present in *both* DataFrames.\n",
    "- **Left join**: keep all rows from the \"left\" DataFrame, and add matching info from the right (non-matching on right become NaN).\n",
    "- **Right join**: keep all rows from the \"right\" DataFrame, and add matching info from the left (non-matching on left become NaN).\n",
    "- **Outer join**: keep all rows from *both* DataFrames, matching where possible (non-matches on either side become NaN).\n",
    "\n",
    "Let's create two DataFrames to demonstrate merging. We'll use a biological example: one table of animal attributes, and another table of observations of those animals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 1: Animal attributes\n",
    "df_animals = pd.DataFrame({\n",
    "    'Animal': ['Lion', 'Zebra', 'Elephant', 'Tiger'],\n",
    "    'Species': ['Panthera leo', 'Equus zebra', 'Loxodonta', 'Panthera tigris'],\n",
    "    'Weight_kg': [190, 300, 5400, 220]  # approximate weights for example\n",
    "})\n",
    "print(\"Animal attributes:\")\n",
    "display(df_animals)\n",
    "\n",
    "# DataFrame 2: Observations (e.g., number of sightings in certain years)\n",
    "df_observations = pd.DataFrame({\n",
    "    'Animal': ['Lion', 'Lion', 'Zebra', 'Tiger', 'Giraffe'],\n",
    "    'Year': [2020, 2021, 2020, 2020, 2021],\n",
    "    'Observations': [5, 3, 7, 2, 1]\n",
    "})\n",
    "print(\"\\nAnimal observations:\")\n",
    "display(df_observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example:\n",
    "- `df_animals` has unique animal entries (Lion, Zebra, Elephant, Tiger) with species name and weight.\n",
    "- `df_observations` has multiple entries per animal (some repeated 'Lion' for different years, etc.), and even an animal ('Giraffe') that is not in `df_animals`.\n",
    "\n",
    "Now, let's merge these DataFrames on the `\"Animal\"` column:\n",
    "- First, do an **inner join** (only animals present in both tables).\n",
    "- Then, a **left join** of `df_animals` with `df_observations` (keep all animals from `df_animals`).\n",
    "- Finally, an **outer join** to see all animals from both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join: only animals that appear in both DataFrames\n",
    "inner_merged = pd.merge(df_animals, df_observations, on='Animal', how='inner')\n",
    "print(\"Inner join result (animals in both tables):\")\n",
    "display(inner_merged)\n",
    "\n",
    "# Left join: all animals from df_animals, add observations if available\n",
    "left_merged = pd.merge(df_animals, df_observations, on='Animal', how='left')\n",
    "print(\"\\nLeft join result (all animals from df_animals):\")\n",
    "display(left_merged)\n",
    "\n",
    "# Outer join: all animals from both DataFrames\n",
    "outer_merged = pd.merge(df_animals, df_observations, on='Animal', how='outer')\n",
    "print(\"\\nOuter join result (all animals from both tables):\")\n",
    "display(outer_merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the results:\n",
    "- **Inner join**: We got Lion, Zebra, Tiger (Elephant was missing observations, Giraffe missing attributes, so they dropped). Only animals present in both data sets are included.\n",
    "- **Left join**: We kept all animals from the left (`df_animals`): Lion, Zebra, Elephant, Tiger. Giraffe (only in observations) is not included. Elephant has NaN for Year and Observations since it had no matching entry.\n",
    "- **Outer join**: We got all animals from both: Lion, Zebra, Elephant, Tiger, Giraffe. Where information was missing on one side, we see NaN (e.g., Elephant has NaN for observations, Giraffe has NaN for species and weight).\n",
    "\n",
    "You can similarly do a right join (`how='right'`), which would keep all from the right DataFrame (`df_observations`) instead.\n",
    "\n",
    "Merging is a crucial tool for combining data. Next, we'll see how to summarize and aggregate data using groupby.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. GroupBy Operations (Aggregation and Custom Functions)\n",
    "\n",
    "**GroupBy** allows you to split the DataFrame into groups based on some criteria, apply an aggregation or transformation to each group, and then combine the results. This follows the \"split-apply-combine\" paradigm:\n",
    "- **Split** the data into groups (by one or more keys).\n",
    "- **Apply** a function to each group (e.g., sum, mean, custom function).\n",
    "- **Combine** the results into a new DataFrame/Series.\n",
    "\n",
    "This is useful in scientific data for computing statistics per group (e.g., average measurements per species, counts per category, etc.).\n",
    "\n",
    "Let's use the `outer_merged` DataFrame from above (which has animals, their attributes, and observations) to demonstrate groupby. We will:\n",
    "- Group by Species and calculate the total and average observations for each species.\n",
    "- Group by Year to see how many observations were recorded each year (regardless of animal).\n",
    "- Show how to apply a custom aggregation function.\n",
    "\n",
    "First, recall our `outer_merged` data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined data (outer_merged):\")\n",
    "display(outer_merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, group the data by **Species** (the species name) and aggregate the Observations:\n",
    "- We'll calculate the **total observations** (`sum`) per species.\n",
    "- Also the **average observations** (`mean`) per species (though with our small dataset, take care with NaNs).\n",
    "\n",
    "We'll use the `.groupby()` method and then `.agg()` to specify multiple aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Species, aggregate observation count\n",
    "species_grouped = outer_merged.groupby('Species')['Observations'].agg(['sum', 'mean'])\n",
    "print(\"Total and average observations per Species:\")\n",
    "display(species_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the sum and mean of observations for each species:\n",
    "- For Panthera leo (Lion) etc.\n",
    "\n",
    "Note: The mean for some species might be based on fewer data points if some years were missing.\n",
    "\n",
    "Next, let's group by **Year** to see the total observations in each year across all animals. This can be done similarly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Year, sum the observations in each year\n",
    "yearly_obs = outer_merged.groupby('Year')['Observations'].sum()\n",
    "print(\"Total observations per Year:\")\n",
    "display(yearly_obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a Series indexed by Year with the total observations for that year.\n",
    "\n",
    "### GroupBy with custom functions\n",
    "\n",
    "You are not limited to built-in aggregations like sum or mean. You can apply custom functions. For example, let's create a custom function to calculate the **range** of observations (max - min) for each group. We will group by Species again for this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom aggregation function: range (max - min)\n",
    "def range_func(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "# Group by Species, apply custom range function to Observations\n",
    "obs_range = outer_merged.groupby('Species')['Observations'].agg(range_func)\n",
    "print(\"Range of observations (max-min) per Species:\")\n",
    "display(obs_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom function was applied to each species' Observations:\n",
    "- For species with only one observation entry, the range is 0.\n",
    "- For those with multiple, it shows the difference between max and min observations.\n",
    "\n",
    "You can also pass multiple functions at once:\n",
    "```python\n",
    "outer_merged.groupby('Species')['Observations'].agg(['sum', 'mean', range_func])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pivot and Pivot Tables (Reshaping Data)\n",
    "\n",
    "Sometimes you'll need to reshape data from a \"long\" format (rows are individual observations) to a \"wide\" format (columns represent categories). Pandas provides:\n",
    "\n",
    "`df.pivot()` for simple reshaping when each index/column pair has a single value.\n",
    "\n",
    "`df.pivot_table()` for situations where you have duplicate entries for an index/column pair and need to aggregate (or if you want to apply an aggregation while pivoting).\n",
    "\n",
    "Think of pivoting like creating a spreadsheet pivot table:\n",
    "\n",
    "You choose an index (rows),\n",
    "\n",
    "a set of columns,\n",
    "\n",
    "and values to fill the table.\n",
    "\n",
    "Let's create an example. Suppose we conducted two types of measurements (speed and strength) for each animal and want to compare them side by side. We can start with a long-format DataFrame and then pivot it.\n",
    "\n",
    "Example long-format data: Animal, Metric, and Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long format data: each animal has multiple metric measurements\n",
    "df_long = pd.DataFrame({\n",
    "    'Animal': ['Lion', 'Lion', 'Zebra', 'Zebra'],\n",
    "    'Metric': ['Speed', 'Strength', 'Speed', 'Strength'],\n",
    "    'Value': [12, 8, 9, 6]\n",
    "})\n",
    "print(\"Long-format data:\")\n",
    "display(df_long)\n",
    "\n",
    "# Pivot this data so that each animal is a row, metrics are columns\n",
    "df_wide = df_long.pivot(index='Animal', columns='Metric', values='Value')\n",
    "print(\"\\nPivoted wide-format data (Animal x Metric):\")\n",
    "display(df_wide)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pivoted the long data:\n",
    "- **index** = Animal (so each animal becomes a row index)\n",
    "- **columns** = Metric (we get a \"Speed\" column and a \"Strength\" column)\n",
    "- **values** = Value (the values to place in the table)\n",
    "\n",
    "The result `df_wide` has animals as rows, and two columns (Speed, Strength) with the corresponding values. Lion's speed 12, strength 8, etc.\n",
    "\n",
    "This worked because in `df_long`, each (Animal, Metric) pair had exactly one value. If there were multiple values for a given Animal and Metric (i.e., duplicates), `pivot()` would not know what to do and would raise an error.\n",
    "\n",
    "For those cases, or when you want to aggregate while pivoting, use `pivot_table`. Let's simulate a scenario with duplicate entries. \n",
    "\n",
    "**Example**: Suppose we have yearly observation counts for species (with potentially multiple counts per species per year, maybe from different observation sources). We'll create a DataFrame with duplicates and use a pivot table to summarize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with potential duplicates: Species observations per Year from different sources\n",
    "df_counts = pd.DataFrame({\n",
    "    'Species': ['Lion', 'Lion', 'Zebra', 'Zebra', 'Zebra', 'Elephant'],\n",
    "    'Year':    [2020, 2020, 2020, 2021, 2021, 2021],\n",
    "    'Count':   [3, 2, 4, 5, 2, 1]   # e.g., two Lion entries in 2020, two Zebra entries in 2021\n",
    "})\n",
    "print(\"Species count data (long format with duplicates):\")\n",
    "display(df_counts)\n",
    "\n",
    "# Attempting to pivot this directly would fail because of duplicate Species-Year pairs.\n",
    "# Instead, use pivot_table to aggregate the counts.\n",
    "species_year_table = df_counts.pivot_table(index='Species', columns='Year', \n",
    "                                          values='Count', aggfunc='sum', fill_value=0)\n",
    "print(\"\\nPivot table (total counts per Species per Year):\")\n",
    "display(species_year_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `pivot_table` with `aggfunc='sum'` to handle duplicate entries by summing them:\n",
    "- Lion in 2020 had two entries (3 and 2), which got summed to 5.\n",
    "- Zebra in 2021 had two entries (5 and 2), summed to 7.\n",
    "- The table shows 0 for combinations where there were no counts (we used `fill_value=0` to replace NaN with 0).\n",
    "\n",
    "Pivot tables are a convenient way to reshape and aggregate data in one go. \n",
    "\n",
    "**Recap:**\n",
    "- Use `pivot` when each index/column pair has a single value.\n",
    "- Use `pivot_table` when you have duplicates or want to apply an aggregation (mean, sum, etc.) during the reshape.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Efficiency Matters: Avoid `DataFrame.apply` for Row-wise Operations\n",
    "\n",
    "A common beginner misconception is to use `df.apply()` for operations on each row, thinking it's vectorized. In reality, `DataFrame.apply` with `axis=1` (row-wise) will call your function for each row in Python, which can be slow for large DataFrames.\n",
    "\n",
    "Pandas (and NumPy underneath) is optimized for **vectorized operations**: operations applied on entire columns (arrays) at once, in C speed. Using a Python loop (explicitly or via `apply` or `iterrows`) breaks this vectorization and can be hundreds of times slower.\n",
    "\n",
    "**Example:** Suppose we have two columns and want to compute a new column as their sum. We'll compare a vectorized approach to an `apply` approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a DataFrame with 10000 rows for demonstration\n",
    "N = 10000\n",
    "df_perf = pd.DataFrame({\n",
    "    'x': np.random.rand(N),\n",
    "    'y': np.random.rand(N)\n",
    "})\n",
    "\n",
    "# Approach 1: Using apply (row by row computation)\n",
    "import time\n",
    "start = time.time()\n",
    "df_perf['sum_apply'] = df_perf.apply(lambda row: row['x'] + row['y'], axis=1)\n",
    "end = time.time()\n",
    "apply_time = end - start\n",
    "\n",
    "# Approach 2: Vectorized operation\n",
    "start = time.time()\n",
    "df_perf['sum_vectorized'] = df_perf['x'] + df_perf['y']\n",
    "end = time.time()\n",
    "vec_time = end - start\n",
    "\n",
    "print(f\"Time using apply: {apply_time:.4f} seconds\")\n",
    "print(f\"Time using vectorized operation: {vec_time:.4f} seconds\")\n",
    "\n",
    "# Verify that results are the same\n",
    "print(\"Results equal?\", df_perf['sum_apply'].equals(df_perf['sum_vectorized']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows how much time each approach took. On a typical run, the apply method is **significantly slower** (even tens of times slower) than the vectorized operation, especially as N grows larger. Yet both methods produce the same result (and we verified the results are equal).\n",
    "\n",
    "For a small DataFrame you might not notice, but for large datasets this difference is huge. **Rule of thumb:** whenever possible, use vectorized operations (column-wise arithmetic, built-in pandas methods) instead of row-wise Python code with `apply`.\n",
    "\n",
    "`df.apply` is not evil—it can be useful for certain column-wise operations or when you truly need to apply a custom function across rows. But always consider if there's a direct pandas or NumPy vectorized way first.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Efficiency Matters: Avoid Loops (`for` or `iterrows`) for DataFrames\n",
    "\n",
    "Similarly to using `apply`, explicitly iterating over rows with a `for` loop or using `DataFrame.iterrows()` is very slow for large DataFrames. Each iteration is pure Python overhead, and `iterrows()` in particular creates a Series for each row (which is additional overhead and may lead to type coercion issues).\n",
    "\n",
    "**Why `iterrows`/loops are slow:**\n",
    "- pandas/NumPy operations run in optimized C code.\n",
    "- A Python loop processes one row at a time in Python, missing out on those optimizations.\n",
    "- `iterrows()` returns copies of data in each row, not a view, which can be inefficient.\n",
    "\n",
    "Let's demonstrate with the same DataFrame `df_perf` by summing columns using a loop vs. vectorized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using iterrows in a loop to sum x and y\n",
    "start = time.time()\n",
    "sums = []\n",
    "for idx, row in df_perf.iterrows():\n",
    "    sums.append(row['x'] + row['y'])\n",
    "df_perf['sum_loop'] = sums\n",
    "end = time.time()\n",
    "loop_time = end - start\n",
    "\n",
    "print(f\"Time using iterrows loop: {loop_time:.4f} seconds\")\n",
    "print(f\"Time using vectorized (from above): {vec_time:.4f} seconds\")\n",
    "print(\"Results equal?\", df_perf['sum_loop'].equals(df_perf['sum_vectorized']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with just 10k rows, the loop is much slower than the vectorized approach. For larger data (100k or 1 million rows), the difference can be dramatic (minutes vs seconds).\n",
    "\n",
    "**Bottom line:** Use pandas vectorized operations or methods (like `.sum()`, `.mean()`, column arithmetic, etc.) whenever possible. If you need to iterate, consider alternatives like:\n",
    "- `DataFrame.itertuples()` (faster than `iterrows`, returns namedtuples) when you must loop in Python.\n",
    "- Use NumPy arrays (access via `df['col'].to_numpy()`) for low-level loops if absolutely necessary.\n",
    "- But best is to rethink the problem in a vectorized way.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Chained Indexing Warning: Avoid Chained Assignments\n",
    "\n",
    "Another common pitfall is **chained indexing**, which means accessing or assigning to a DataFrame using multiple indexing operations in one go (like `df[mask]['col'] = value`). This can lead to the dreaded `SettingWithCopyWarning` in pandas, or even silently not do what you expect.\n",
    "\n",
    "**What is chained indexing?**  \n",
    "It's when you index into a DataFrame, and then index the result again, e.g.:\n",
    "```python\n",
    "df[df['Animal'] == 'Lion']['Speed'] = 5\n",
    "```\n",
    "\n",
    "In this example, df[df['Animal'] == 'Lion'] creates a new DataFrame (a copy of part of the data). Then ['Speed'] = 5 tries to set values on that new object, not the original df. This may either do nothing to df or raise a warning. Pandas warns you because this kind of operation is ambiguous (sometimes it might work on a view of the original data, other times on a copy).\n",
    "\n",
    "How to avoid it:\n",
    "\n",
    "- Use single-step indexing with .loc for setting values, which ensures pandas sets on the original DataFrame.\n",
    "- Alternatively, break it into two steps with an explicit temporary variable if needed (first filtering, then setting on the filtered copy – but in that case, use that copy knowingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a simple DataFrame for demonstration\n",
    "df_temp = pd.DataFrame({\n",
    "    'Animal': ['Lion', 'Zebra', 'Lion', 'Zebra'],\n",
    "    'Speed': [10, 12, 11, 9]\n",
    "})\n",
    "print(\"Before chained assignment:\")\n",
    "display(df_temp)\n",
    "\n",
    "# Attempt a chained indexing assignment (this might produce a warning)\n",
    "import warnings\n",
    "warnings.filterwarnings('error')  # To turn SettingWithCopyWarning into an error for demonstration\n",
    "try:\n",
    "    df_temp[df_temp['Animal'] == 'Lion']['Speed'] = 0  # Trying to set Speed=0 for all Lions (incorrect way)\n",
    "except Warning as w:\n",
    "    print(f\"Warning or Error caught: {type(w).__name__}: {w}\")\n",
    "\n",
    "# Proper way: use .loc for conditional assignment\n",
    "df_temp.loc[df_temp['Animal'] == 'Lion', 'Speed'] = 0\n",
    "print(\"\\nAfter .loc assignment (set Speed=0 for all Lions):\")\n",
    "display(df_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example:\n",
    "- The chained indexing attempt raised a warning/error (as expected) because it was a two-step setting.\n",
    "- Using `.loc` in one step (`df_temp.loc[condition, 'Speed'] = 0`) correctly modified the original DataFrame.\n",
    "\n",
    "When you see a `SettingWithCopyWarning`, it usually means you're doing chained indexing. Always prefer `.loc` for setting values based on conditions or index labels.\n",
    "\n",
    "---\n",
    "\n",
    "Now that we've covered these core pandas concepts (multi-indexing, merging, groupby, pivoting, and writing efficient pandas code), it's time to practice with a couple of challenges!\n",
    "\n",
    "## Challenge 1: Combining Techniques (Merge, GroupBy, Pivot)\n",
    "\n",
    "**Scenario:** You have two datasets:\n",
    "- One contains information about individual animals (their species, etc.).\n",
    "- Another contains yearly observations of those animals.\n",
    "\n",
    "Your task is to **merge** these datasets and then **summarize** the observations by species and year.\n",
    "\n",
    "**Steps to perform:**\n",
    "1. **Merge** the two DataFrames on the animal identifier (e.g., \"Animal\" name) to get a combined dataset with species and yearly observations.\n",
    "2. Use **groupby** (or directly pivot_table) to calculate the total observations per Species for each Year.\n",
    "3. **Pivot** the result so that Species are rows and Years are columns (so we can easily see a table of species vs year observations). Fill missing combinations with 0.\n",
    "4. (Bonus) Try doing it in one step with `pivot_table` for comparison.\n",
    "\n",
    "We've actually prepared example data for you below. Fill in the code where needed to perform the tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1 Data\n",
    "df_info = pd.DataFrame({\n",
    "    'Animal': ['A001', 'A002', 'A003', 'A004'],\n",
    "    'Species': ['Lion', 'Lion', 'Zebra', 'Elephant']\n",
    "})\n",
    "df_obs = pd.DataFrame({\n",
    "    'Animal': ['A001', 'A001', 'A002', 'A003', 'A003', 'A004'],\n",
    "    'Year':   [2020,   2021,   2020,   2020,   2021,   2021],\n",
    "    'Observations': [5, 3, 2, 4, 7, 1]\n",
    "})\n",
    "print(\"Animal info:\")\n",
    "display(df_info)\n",
    "print(\"Yearly observations:\")\n",
    "display(df_obs)\n",
    "\n",
    "# 1. Merge the data on 'Animal'\n",
    "#df_merged =\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "display(df_merged)\n",
    "\n",
    "# 2. Group by Species and Year, summing the observations\n",
    "#species_year_grouped = \n",
    "print(\"\\nTotal Observations by Species and Year (GroupBy result):\")\n",
    "display(species_year_grouped)\n",
    "\n",
    "# (Optional) reset index to see it clearly as columns\n",
    "#species_year_df = \n",
    "print(\"\\nAfter resetting index (Species, Year are columns):\")\n",
    "display(species_year_df)\n",
    "\n",
    "# 3. Pivot the grouped data so that Species are index and Year are columns\n",
    "#species_year_pivot = \n",
    "print(\"\\nPivoted table (Species x Year):\")\n",
    "display(species_year_pivot)\n",
    "\n",
    "# 4. Alternatively, one-step pivot_table (for verification)\n",
    "#species_year_pivot2 = \n",
    "print(\"\\nPivot table (one-step) result, should match the above:\")\n",
    "display(species_year_pivot2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "- The merged DataFrame (`df_merged`) should contain each observation entry with its species.\n",
    "- The groupby result (`species_year_grouped`) should show totals like: Elephant 2020 = 0 (none), Elephant 2021 = 1, Lion 2020 = 7 (A001 5 + A002 2), Lion 2021 = 3, Zebra 2020 = 4, Zebra 2021 = 7.\n",
    "- The pivoted table will have Species as rows (Elephant, Lion, Zebra) and columns for Year 2020, 2021 with those totals. For example, Lion row would be [2020: 7, 2021: 3], Zebra [2020:4, 2021:7], Elephant [2020:0, 2021:1].\n",
    "\n",
    "Great job! Now you have combined merging, grouping, and pivoting to derive a summary table.\n",
    "\n",
    "## Challenge 2: Writing Efficient Pandas Code\n",
    "\n",
    "In this challenge, you'll refactor some inefficient code into an idiomatic pandas solution.\n",
    "\n",
    "**Scenario:** We have a DataFrame of individuals with their weight (in kilograms) and height (in meters). We want to compute a new column for Body Mass Index (BMI = weight / height^2).\n",
    "\n",
    "The initial code is written in a slow way using `.apply`. Your task is to rewrite it without using `.apply` or any explicit loops, i.e., using vectorized operations.\n",
    "\n",
    "**Steps:**\n",
    "1. Run the provided code to see the result and confirm the correct BMI calculation.\n",
    "2. Implement a vectorized solution to calculate the BMI and verify it matches the apply method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2 Data\n",
    "df_health = pd.DataFrame({\n",
    "    'Weight_kg': [70, 80, 60, 90],   # weight of 4 individuals\n",
    "    'Height_m': [1.75, 1.8, 1.6, 1.9]  # height of 4 individuals\n",
    "})\n",
    "print(\"Health DataFrame:\")\n",
    "display(df_health)\n",
    "\n",
    "# Inefficient approach: using apply to compute BMI for each row\n",
    "df_health['BMI_apply'] = df_health.apply(lambda row: row['Weight_kg'] / (row['Height_m'] ** 2), axis=1)\n",
    "print(\"\\nAfter apply (BMI_apply):\")\n",
    "display(df_health)\n",
    "\n",
    "# 2. Efficient vectorized approach: compute BMI without apply (fill in this line)\n",
    "#df_health['BMI_vectorized'] = \n",
    "print(\"\\nAfter vectorized computation (BMI_vectorized):\")\n",
    "display(df_health)\n",
    "\n",
    "# Check that both BMI columns are the same\n",
    "print(\"Difference between BMI_apply and BMI_vectorized:\")\n",
    "print(df_health['BMI_apply'] - df_health['BMI_vectorized'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "- The `BMI_vectorized` column should match the `BMI_apply` column exactly for all rows.\n",
    "- The difference printed at the end should be all zeros (or very close to zero, considering floating-point precision).\n",
    "\n",
    "By replacing the apply with a direct arithmetic operation on columns, we've made the code shorter and much faster (imagine if this DataFrame had thousands of rows—the vectorized version would shine in comparison)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
